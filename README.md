# Lunar Lander PPO å¼·åŒ–å­¦ç¿’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  **PPOï¼ˆProximal Policy Optimizationï¼‰** ã‚’ä½¿ç”¨ã—ã¦ã€`LunarLander-v3` ç’°å¢ƒã§ç€é™¸åˆ¶å¾¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚  
ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã¯ [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3) ã¨ [Gymnasium](https://gymnasium.farama.org/) ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚


## ğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç›®çš„

æœˆé¢ç€é™¸ã‚’æ¨¡ã—ãŸ `LunarLander` ç’°å¢ƒã«ãŠã„ã¦ã€å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå®‰å…¨ã«ç€é™¸ã™ã‚‹å‹•ä½œã‚’è‡ªå‹•çš„ã«ç²å¾—ã§ãã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã›ã¾ã™ã€‚


## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ
```
.
â”œâ”€â”€ README.md                  # æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
â”œâ”€â”€ environment.yml            # Conda ç’°å¢ƒãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ train_lunar_lander.py     # å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆPPOã§è¨“ç·´ï¼‰
â”œâ”€â”€ evaluate_lunar_lander.py  # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ spot_simulation.gif       # è©•ä¾¡çµæœã®GIFï¼ˆREADMEç”¨ã«åŸ‹ã‚è¾¼ã¿å¯èƒ½ï¼‰
â”œâ”€â”€ lunar-lander-ppo-episode-5.mp4  # è©•ä¾¡çµæœã®å‹•ç”»ï¼ˆé«˜ç”»è³ªä¿å­˜ï¼‰

```

## ğŸ› ï¸ å®Ÿè£…æ‰‹é †

ä»¥ä¸‹ã®æ‰‹é †ã‚’ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§è¡Œã£ã¦ãã ã•ã„ã€‚

### 1. ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³

```bash
git clone https://github.com/Kingkazu106/lunar_lander_RL.git
cd lunar_lander_RL
```
### 2. ç’°å¢ƒã®æ§‹ç¯‰
ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¯ `environment.yml` ãŒä»˜å±ã—ã¦ã„ã¾ã™ã€‚

```bash
conda env create -f environment.yml
conda activate lunar_lander_env
```

### 3. å­¦ç¿’ã®å®Ÿè¡Œï¼ˆãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ï¼‰
```bash
python train_lunar_lander.py
```
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ `ppo_lunar_lander.zip` ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¾ã™ã€‚
å­¦ç¿’ãƒ­ã‚°ã¯ `lunar_lander_tensorboard/` ã«å‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

### 4. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ï¼ˆå‹•ä½œç¢ºèªï¼‰
```bash
python evaluate_lunar_lander.py
```
è©•ä¾¡çµæœã®å‹•ç”»`lunar-lander-ppo-episode-5.mp4`ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚
## ğŸ¥å‡ºåŠ›ä¾‹

![Spot Simulation](https://github.com/Kingkazu106/lunar_lander_RL/blob/main/spot_simulation.gif)


# ğŸŒŒ PPO ã«ã‚ˆã‚‹ LunarLander ã®å¼·åŒ–å­¦ç¿’

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ŒProximal Policy Optimizationï¼ˆPPOï¼‰ã€ã‚’ç”¨ã„ã¦ã€LunarLander ç’°å¢ƒã«ãŠã‘ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆ¶å¾¡ã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚



## ğŸ“˜ èƒŒæ™¯ï¼šãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰

å¼·åŒ–å­¦ç¿’ã¯ã€**ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ï¼ˆMDPï¼‰** ã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«åŒ–ã•ã‚Œã¾ã™ã€‚

- çŠ¶æ…‹ç©ºé–“ï¼š$s \in \mathcal{S}$
- è¡Œå‹•ç©ºé–“ï¼š$a \in \mathcal{A}$
- é·ç§»ç¢ºç‡ï¼š$P(s' \mid s, a)$
- å ±é…¬é–¢æ•°ï¼š$r(s, a)$
- å‰²å¼•ç‡ï¼š$\gamma \in [0, 1)$

ç›®çš„ã¯ã€**æœŸå¾…ç´¯ç©å ±é…¬**ã‚’æœ€å¤§åŒ–ã™ã‚‹æ–¹ç­– $\pi(a \mid s)$ ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã™ï¼š

$$
J(\pi) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]
$$



## ğŸ¤– PPOï¼ˆProximal Policy Optimizationï¼‰

PPOã¯ã€æ–¹ç­–å‹¾é…æ³•ã«ãŠã„ã¦å®‰å®šæ€§ã¨åŠ¹ç‡ã‚’ä¸¡ç«‹ã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚  
å¾“æ¥ã®æ–¹ç­– $\pi_{\theta_{old}}$ ã¨æ–°ã—ã„æ–¹ç­– $\pi_\theta$ ã®æ¯”ç‡ï¼š

$$
r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{old}}(a_t \mid s_t)}
$$

PPOã®ç›®çš„é–¢æ•°ã¯ã€ä»¥ä¸‹ã®ã‚¯ãƒªãƒƒãƒ—ä»˜ãæå¤±é–¢æ•°ã‚’æœ€å¤§åŒ–ã—ã¾ã™ï¼š

$$
L^{CLIP}(\theta) = \mathbb{E}_t \left[ 
\min\left( 
r_t(\theta) \hat{A}_t, 
\text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t 
\right) 
\right]
$$

ã“ã“ã§ã€$\hat{A}_t$ ã¯ Advantageï¼ˆä¾¡å€¤é–¢æ•°ã¨å®Ÿå ±é…¬ã®å·®ï¼‰ã§ã™ã€‚

---

## âš™ï¸ å­¦ç¿’ã®è¨­å®š

```python
env = make_vec_env('LunarLander-v3', n_envs=16)

model = PPO(
    'MlpPolicy', 
    env, 
    n_steps=1024,
    batch_size=64,
    n_epochs=10,
    learning_rate=0.0001,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2
)
